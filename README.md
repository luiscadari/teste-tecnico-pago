# API Crawler de CEPs - Desafio TÃ©cnico

Sistema de crawler assÃ­ncrono para consulta em massa de CEPs utilizando a API ViaCEP, desenvolvido como resoluÃ§Ã£o de desafio tÃ©cnico.

## ğŸ“‹ Sobre o Projeto

Esta aplicaÃ§Ã£o permite realizar consultas em massa de CEPs de forma assÃ­ncrona, processando requisiÃ§Ãµes atravÃ©s de filas SQS (Amazon Simple Queue Service) e armazenando os resultados em MongoDB.

### Funcionalidades

- **Crawler AssÃ­ncrono**: Processa intervalos de CEPs de forma nÃ£o bloqueante
- **Sistema de Filas**: Utiliza AWS SQS (ElasticMQ local) para gerenciamento de requisiÃ§Ãµes
- **Armazenamento Persistente**: MongoDB para guardar histÃ³rico e resultados
- **Monitoramento**: Interface Mongo Express para visualizaÃ§Ã£o dos dados
- **API RESTful**: Endpoints para iniciar crawlers e consultar status/resultados

## ğŸš€ Tecnologias Utilizadas

- **Node.js** - Runtime JavaScript
- **Express** - Framework web
- **MongoDB** - Banco de dados NoSQL
- **Mongoose** - ODM para MongoDB
- **AWS SDK** - IntegraÃ§Ã£o com SQS
- **ElasticMQ** - Simulador local de SQS
- **Docker & Docker Compose** - ContainerizaÃ§Ã£o
- **Axios** - Cliente HTTP
- **ViaCEP API** - Fonte de dados de CEPs

## ğŸ“¦ Estrutura do Projeto

```
teste-tecnico-pago/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ sqsClient.js           # ConfiguraÃ§Ã£o do cliente SQS
â”‚   â”œâ”€â”€ controller/
â”‚   â”‚   â””â”€â”€ cep.contrroller.js     # Controladores das rotas
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â””â”€â”€ index.js               # ConexÃ£o com MongoDB
â”‚   â”œâ”€â”€ router/
â”‚   â”‚   â”œâ”€â”€ cep.router.js          # Rotas da API
â”‚   â”‚   â””â”€â”€ index.js               # Agregador de rotas
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â””â”€â”€ crawler.schema.js      # Schema do MongoDB
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ cep.services.js        # LÃ³gica de negÃ³cio
â”‚       â””â”€â”€ queue.services.js      # Gerenciamento de filas
â”œâ”€â”€ docker-compose.yaml            # ConfiguraÃ§Ã£o dos containers
â”œâ”€â”€ index.js                       # Ponto de entrada da aplicaÃ§Ã£o
â””â”€â”€ package.json                   # DependÃªncias e scripts
```

## ğŸ”§ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

### PrÃ©-requisitos

- Docker
- Docker Compose
- Node.js 22+ (opcional, para desenvolvimento local)

### Executando com Docker (Recomendado)

1. Clone o repositÃ³rio:

```bash
git clone <url-do-repositorio>
cd teste-tecnico-pago
```

2. Inicie os containers:

```bash
docker-compose up
```

A aplicaÃ§Ã£o estarÃ¡ disponÃ­vel em:

- **API**: http://localhost:8888
- **Mongo Express**: http://localhost:8081 (usuÃ¡rio: `mongoexpressuser`, senha: `mongoexpresspass`)

### Executando Localmente

1. Instale as dependÃªncias:

```bash
npm install
```

2. Configure as variÃ¡veis de ambiente (criar arquivo `.env`):

```env
PORT=3000
MONGO_USERNAME=root
MONGO_PASSWORD=example
MONGO_HOST=localhost
MONGO_PORT=27017
QUEUE_URL=http://localhost:9324/queue/cep-queue
MAX_NUMBER_OF_MESSAGES=10
REQUEST_DELAY_MS=1000
```

3. Inicie apenas MongoDB e ElasticMQ:

```bash
docker-compose up mongo elasticmq mongo-express
```

4. Execute a aplicaÃ§Ã£o:

```bash
npm run dev    # Modo desenvolvimento
npm start      # Modo produÃ§Ã£o
```

## ğŸ“š API Endpoints

### POST /crawler

Inicia um novo processo de crawling de CEPs.

**Request Body:**

```json
{
  "cep_start": "01001000",
  "cep_end": "01001100"
}
```

**Response:**

```json
{
  "crawlerId": "507f1f77bcf86cd799439011"
}
```

### GET /crawler/:id

Consulta o status de um crawler especÃ­fico.

**Response:**

```json
{
  "total": 101,
  "processed": 50,
  "successful": 45,
  "failed": 5,
  "status": "pending"
}
```

### GET /crawler/:id/results

Retorna todos os resultados coletados por um crawler.

**Response:**

```json
[
  {
    "cep": "01001000",
    "logradouro": "PraÃ§a da SÃ©",
    "bairro": "SÃ©",
    "localidade": "SÃ£o Paulo",
    "uf": "SP",
    "error": null
  },
  ...
]
```

## ğŸ¯ Arquitetura e Fluxo

1. **RequisiÃ§Ã£o Inicial**: Cliente envia intervalo de CEPs via POST `/crawler`
2. **CriaÃ§Ã£o de Jobs**: Sistema divide o intervalo em jobs individuais
3. **Enfileiramento**: Cada CEP Ã© adicionado Ã  fila SQS
4. **Processamento AssÃ­ncrono**: Worker processa mensagens da fila
5. **Consulta API**: Realiza requisiÃ§Ã£o para ViaCEP
6. **Armazenamento**: Salva resultados no MongoDB
7. **Consulta**: Cliente pode verificar status e resultados via GET

## âš™ï¸ ConfiguraÃ§Ãµes

### VariÃ¡veis de Ambiente

| VariÃ¡vel                 | DescriÃ§Ã£o                    | PadrÃ£o                                |
| ------------------------ | ---------------------------- | ------------------------------------- |
| `PORT`                   | Porta da aplicaÃ§Ã£o           | 8888                                  |
| `MONGO_USERNAME`         | UsuÃ¡rio do MongoDB           | root                                  |
| `MONGO_PASSWORD`         | Senha do MongoDB             | example                               |
| `MONGO_HOST`             | Host do MongoDB              | mongo                                 |
| `MONGO_PORT`             | Porta do MongoDB             | 27017                                 |
| `QUEUE_URL`              | URL da fila SQS              | http://elasticmq:9324/queue/cep-queue |
| `MAX_NUMBER_OF_MESSAGES` | MÃ¡x. de mensagens por lote   | 10                                    |
| `REQUEST_DELAY_MS`       | Delay entre requisiÃ§Ãµes (ms) | 1000                                  |

## ğŸ§ª Testando a AplicaÃ§Ã£o

### Exemplo de Uso

```bash
# Iniciar crawler
curl -X POST http://localhost:8888/crawler \
  -H "Content-Type: application/json" \
  -d '{"cep_start": "01001000", "cep_end": "01001010"}'

# Verificar status (substitua o ID)
curl http://localhost:8888/crawler/507f1f77bcf86cd799439011

# Obter resultados
curl http://localhost:8888/crawler/507f1f77bcf86cd799439011/results
```

## ğŸ“Š Monitoramento

Acesse o Mongo Express em http://localhost:8081 para:

- Visualizar documentos salvos
- Verificar status dos crawlers
- Analisar resultados em tempo real

## ğŸ”’ ConsideraÃ§Ãµes de SeguranÃ§a

âš ï¸ **Este projeto Ã© para fins de demonstraÃ§Ã£o tÃ©cnica.** Em produÃ§Ã£o, considere:

- Implementar autenticaÃ§Ã£o e autorizaÃ§Ã£o
- Adicionar rate limiting
- Validar e sanitizar entradas
- Utilizar HTTPS
- Proteger credenciais sensÃ­veis
- Configurar AWS SQS real com IAM roles

## ğŸ¤ Contribuindo

Este Ã© um projeto de desafio tÃ©cnico, mas sugestÃµes sÃ£o bem-vindas.

## ğŸ‘¤ Autor

**luiscadari**

---

Desenvolvido como resoluÃ§Ã£o de desafio tÃ©cnico para vaga de Desenvolvedor Pleno.
